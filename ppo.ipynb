{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def get_action(self, x):\n",
    "        out = self(x)\n",
    "        action = torch.distributions.Categorical(out).sample()\n",
    "        return action\n",
    "\n",
    "class ValueModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_returns(rewards, gamma):\n",
    "    # rewards: shape [T]\n",
    "    T = rewards.shape[0]\n",
    "    device = rewards.device\n",
    "\n",
    "    indices = torch.arange(T, device=device)\n",
    "    # Create a T x T grid of indices\n",
    "    j_mat, i_mat = torch.meshgrid(indices, indices, indexing='ij')\n",
    "\n",
    "    # Mask for upper-triangular (including diagonal): j >= i\n",
    "    mask = (j_mat > i_mat)\n",
    "\n",
    "    # Compute exponents for gamma^(j - i)\n",
    "    exps = i_mat - j_mat\n",
    "    exps += torch.tensor(1e8, dtype=torch.long) * mask\n",
    "\n",
    "    # Construct the discount matrix G\n",
    "    G = gamma ** exps\n",
    "\n",
    "    # Compute the discounted returns R = G @ rewards\n",
    "    return G @ rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset for episodes and additional tensors\n",
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(self, all_episodes_obs, all_episodes_aux, base_probs, base_advantages, rtgs):\n",
    "        self.all_episodes_obs = all_episodes_obs\n",
    "        self.all_episodes_aux = all_episodes_aux\n",
    "        self.base_probs = base_probs\n",
    "        self.base_advantages = base_advantages\n",
    "        self.rtgs = rtgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_episodes_obs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_episodes_obs[idx], \n",
    "                self.all_episodes_aux[idx], \n",
    "                self.base_probs[idx], \n",
    "                self.base_advantages[idx], \n",
    "                self.rtgs[idx])\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return (list([item[0] for item in batch]), \n",
    "                list([item[1] for item in batch]), \n",
    "                list([item[2] for item in batch]), \n",
    "                list([item[3] for item in batch]), \n",
    "                list([item[4] for item in batch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma=0.99, gae_lambda=0.95, epsilon=0.2, lr=0.0001, env_name=\"CartPole-v1\"):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self.policy_model = PolicyModel()\n",
    "        self.value_model = ValueModel()\n",
    "        self.policy_optimizer = optim.Adam(list(self.policy_model.parameters()), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(list(self.value_model.parameters()), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def update(self, policy_loss = None, value_loss = None):\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.policy_optimizer.step()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "    \n",
    "    def run_episode(self, env_name=\"CartPole-v1\"):\n",
    "        # records state, action, reward for each step\n",
    "        env = gym.make(env_name)  # Remove render_mode for training\n",
    "        episode_obs = torch.tensor([])\n",
    "        episode_aux = torch.tensor([])\n",
    "        observation, info = env.reset()\n",
    "        obs_output = torch.tensor(observation, dtype=torch.float32)[None, :]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not terminated and not truncated:\n",
    "            obs_input = obs_output\n",
    "            action = self.policy_model.get_action(obs_input)\n",
    "            observation, reward, terminated, truncated, info = env.step(action.numpy()[0])\n",
    "            obs_output = torch.tensor(observation, dtype=torch.float32)[None, :]\n",
    "            episode_obs = torch.cat([episode_obs, torch.cat((obs_input, obs_output))[None, :]])\n",
    "            episode_aux = torch.cat([episode_aux, torch.tensor([action, reward])[None, :]])\n",
    "        return episode_obs, episode_aux\n",
    "\n",
    "    def get_losses(self, states, actions, base_probs, base_advantages, real_rtg, epsilon=0.2):\n",
    "        mse = nn.MSELoss()\n",
    "        value_loss = mse(self.value_model(states)[:, 0], real_rtg)\n",
    "\n",
    "        curr_probs = self.policy_model(states)[torch.arange(len(states)), actions.to(torch.int64)]\n",
    "        clipped_weighted_advantages = base_advantages * torch.clip(curr_probs/base_probs, 1-epsilon, 1+epsilon)\n",
    "        weighted_advantages = base_advantages * curr_probs/base_probs\n",
    "        policy_loss = -torch.min(clipped_weighted_advantages, weighted_advantages).mean()\n",
    "        return policy_loss.mean(), value_loss\n",
    "\n",
    "    def compute_statistics(self, all_episodes_obs, all_episodes_aux):\n",
    "        base_probs = []\n",
    "        base_advantages = []\n",
    "        rtgs = []\n",
    "        for episode_obs, episode_aux in zip(all_episodes_obs, all_episodes_aux):\n",
    "            base_probs.append(self.policy_model(episode_obs[:, 0])[torch.arange(episode_obs.shape[0]), episode_aux[:, 0].to(torch.int64)].detach())\n",
    "\n",
    "            td_error = episode_aux[:, 1] + self.gamma * self.value_model(episode_obs[:, 1])[:,0] - self.value_model(episode_obs[:, 0])[:,0]\n",
    "            gae_schedule = (self.gae_lambda * self.gamma) ** torch.arange(episode_obs.shape[0])\n",
    "            new_base_advantage = (td_error * gae_schedule).flip(dims=[0]).cumsum(dim=0).flip(dims=[0]) / gae_schedule\n",
    "            base_advantages.append(new_base_advantage.detach())\n",
    "\n",
    "            gamma_schedule = self.gamma ** torch.arange(episode_obs.shape[0])\n",
    "            real_rtg = (episode_aux[:, 1] * gamma_schedule).flip(dims=[0]).cumsum(dim=0).flip(dims=[0]) / gamma_schedule\n",
    "            rtgs.append(real_rtg.detach())\n",
    "        base_probs = torch.cat(base_probs)\n",
    "        base_advantages = torch.cat(base_advantages)\n",
    "        rtgs = torch.cat(rtgs)\n",
    "        return base_probs, base_advantages, rtgs\n",
    "        \n",
    "        \n",
    "    def ppo_update(self, all_episodes_obs, all_episodes_aux, steps=4, batch_size=32):\n",
    "        base_probs, base_advantages, rtgs = self.compute_statistics(all_episodes_obs, all_episodes_aux)\n",
    "        \n",
    "        # Create a DataLoader for mini-batching\n",
    "        dataset = EpisodeDataset(torch.cat(all_episodes_obs).tolist(), torch.cat(all_episodes_aux).tolist(), base_probs.tolist(), base_advantages.tolist(), rtgs.tolist())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            for states, aux, base_prob, base_adv, rtgs in dataloader:\n",
    "                policy_loss, value_loss = self.get_losses(torch.tensor(states)[:, 0], torch.tensor(aux)[:, 0], torch.tensor(base_prob), torch.tensor(base_adv), torch.tensor(rtgs))\n",
    "                self.update(policy_loss=policy_loss, value_loss=value_loss)\n",
    "        \n",
    "        return policy_loss, value_loss\n",
    "\n",
    "    def avg_reward(self, episodes):\n",
    "        return torch.tensor([episode[1][:, 1].sum() for episode in episodes]).mean()\n",
    "\n",
    "    def train(self, num_episodes=100, print_loss=True):\n",
    "        # collects episodes, updates policy and value models\n",
    "        all_episodes = []\n",
    "        for i in range(num_episodes):\n",
    "            episode = self.run_episode()\n",
    "            all_episodes.append(episode)\n",
    "        \n",
    "        all_episodes_obs = [episode[0] for episode in all_episodes]\n",
    "        all_episodes_aux = [episode[1] for episode in all_episodes]\n",
    "        policy_loss, value_loss = self.ppo_update(all_episodes_obs, all_episodes_aux)\n",
    "        total_reward = self.avg_reward(all_episodes).item()\n",
    "\n",
    "        if print_loss:\n",
    "            print(f\"Episode {i} policy loss: {policy_loss.item()}\")\n",
    "            print(f\"Episode {i} value loss: {value_loss.item()}\")\n",
    "            print(f\"Episode {i} average total reward: {total_reward}\")\n",
    "        return (policy_loss, value_loss, total_reward)\n",
    "    \n",
    "    def demo(self, env_name=\"CartPole-v1\"):\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "        observation, info = env.reset()\n",
    "        obs_output = torch.tensor(observation, dtype=torch.float32)[None, :]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not terminated and not truncated:\n",
    "            action = self.policy_model.get_action(obs_output)\n",
    "            observation, reward, terminated, truncated, info = env.step(action.numpy()[0])\n",
    "            obs_output = torch.tensor(observation, dtype=torch.float32)[None, :]\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 31/1000 [02:00<1:57:49,  7.30s/it]"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "total_rewards = []\n",
    "for i in tqdm(range(100), desc=\"Training\"):\n",
    "    policy_loss, value_loss, total_reward = agent.train(num_episodes=40, print_loss=False)\n",
    "    policy_losses.append(policy_loss)\n",
    "    value_losses.append(value_loss)\n",
    "    total_rewards.append(total_reward)\n",
    "    if total_reward > 500:\n",
    "        print(f\"Episode {i} average total reward: {total_reward}\")\n",
    "        break\n",
    "\n",
    "agent.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/22/ff9c0t7s29vfz_hk8wdcgkcc0000gn/T/ipykernel_60265/3961413046.py\u001b[0m(81)\u001b[0;36mppo_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     79 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     80 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 81 \u001b[0;31m                \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrtgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     82 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     83 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[[[-0.1176726296544075, -1.022016167640686, 0.1543235033750534, 1.5286890268325806], [-0.13811294734477997, -0.829055666923523, 0.1848972886800766, 1.287879228591919]], [[0.4113537073135376, 0.8845100998878479, 0.1301078051328659, -0.043131254613399506], [0.42904388904571533, 0.6877857446670532, 0.12924517691135406, 0.28760507702827454]], [[-0.06013728305697441, -0.61220782995224, -0.0020889299921691418, 0.6747570037841797], [-0.07238143682479858, -0.4170569181442261, 0.011406210251152515, 0.3814171254634857]], [[0.025346040725708008, -0.24241144955158234, -0.005178818479180336, 0.2937220335006714], [0.02049781195819378, -0.43745917081832886, 0.0006956223514862359, 0.5847671627998352]], [[0.02049781195819378, -0.43745917081832886, 0.0006956223514862359, 0.5847671627998352], [0.01174862775951624, -0.24234698712825775, 0.012390965595841408, 0.2923034429550171]], [[-0.02127034030854702, 0.7492538690567017, 0.04697170853614807, -0.7982932329177856], [-0.006285262294113636, 0.5535200238227844, 0.031005846336483955, -0.4912117123603821]], [[0.014743158593773842, -0.15299184620380402, -0.04017126187682152, 0.2862589657306671], [0.011683321557939053, -0.34751856327056885, -0.03444608300924301, 0.5660064816474915]], [[-0.03615586832165718, 0.0322110652923584, -0.016569918021559715, 0.00048606388736516237], [-0.035511646419763565, 0.2275666892528534, -0.01656019501388073, -0.2973784804344177]], [[0.10657499730587006, -0.32295188307762146, -0.1615389734506607, -0.00576000614091754], [0.10011596232652664, -0.12592627108097076, -0.16165417432785034, -0.3447369933128357]], [[0.01305046770721674, 0.6108037829399109, -0.12023784220218658, -1.0044329166412354], [0.02526654303073883, 0.4174751341342926, -0.14032649993896484, -0.7517984509468079]], [[-0.005391970276832581, 0.0102922972291708, 0.03484470769762993, 0.057377152144908905], [-0.005186124239116907, -0.18531149625778198, 0.035992249846458435, 0.36084699630737305]], [[-0.12918902933597565, -1.1990095376968384, 0.077694371342659, 1.5875625610351562], [-0.15316922962665558, -1.004892110824585, 0.10944562405347824, 1.3200838565826416]], [[-0.044774554669857025, 0.18261094391345978, 0.0018533545080572367, -0.35463884472846985], [-0.04112233594059944, -0.012537305243313313, -0.005239422433078289, -0.06137208640575409]], [[0.028951073065400124, -0.15704362094402313, -0.008555044420063496, 0.3063010573387146], [0.025810200721025467, 0.03819919377565384, -0.0024290229193866253, 0.010932418517768383]], [[0.05819249525666237, 0.4332199692726135, -0.16257455945014954, -0.8322362899780273], [0.06685689836740494, 0.24064849317073822, -0.17921927571296692, -0.5947746634483337]], [[-0.041373081505298615, 0.18265937268733978, -0.006466864142566919, -0.3557034730911255], [-0.03771989420056343, 0.37787267565727234, -0.013580934144556522, -0.6504185199737549]], [[0.13251835107803345, 1.0166295766830444, -0.14857318997383118, -1.539900302886963], [0.152850940823555, 0.8235742449760437, -0.1793711930513382, -1.297027587890625]], [[0.11972705274820328, 0.24202418327331543, -0.1335168182849884, -0.4776691794395447], [0.12456753849983215, 0.43875354528427124, -0.14307020604610443, -0.8092743158340454]], [[-0.04760332405567169, -0.03800056129693985, 0.0011452989419922233, 0.04770675674080849], [-0.0483633354306221, -0.23313891887664795, 0.002099434146657586, 0.3407508134841919]], [[-0.14140407741069794, -0.9538165926933289, 0.15969672799110413, 1.6077613830566406], [-0.16048039495944977, -1.1504253149032593, 0.19185195863246918, 1.9456733465194702]], [[0.024934057146310806, 0.02816152013838291, -0.002823563991114497, 0.04638150334358215], [0.02549728751182556, 0.22332385182380676, -0.0018959339940920472, -0.2471909373998642]], [[0.09199937433004379, 0.4223136007785797, -0.09179292619228363, -0.6274926066398621], [0.10044564306735992, 0.22858458757400513, -0.10434277355670929, -0.36507174372673035]], [[-0.006245085038244724, 0.037939004600048065, -0.006191676948219538, 0.050819750875234604], [-0.005486304871737957, -0.1570936143398285, -0.005175281781703234, 0.3415427505970001]], [[-0.040379952639341354, 0.22825677692890167, -0.053237058222293854, -0.36596930027008057], [-0.035814814269542694, 0.03393018990755081, -0.06055644527077675, -0.0905366837978363]], [[-0.12360591441392899, -0.953603982925415, 0.17771393060684204, 1.5078786611557007], [-0.14267799258232117, -0.761025607585907, 0.20787149667739868, 1.2755340337753296]], [[-0.12454381585121155, -0.33427220582962036, -0.06346406787633896, -0.2084473967552185], [-0.13122926652431488, -0.13830289244651794, -0.0676330178976059, -0.5204554796218872]], [[0.06433160603046417, -0.18946781754493713, 0.0005686330841854215, 0.23801694810390472], [0.06054225191473961, -0.38459789752960205, 0.0053289723582565784, 0.5308791995048523]], [[-0.08387161791324615, -0.5813034176826477, 0.17720860242843628, 1.1481974124908447], [-0.09549768269062042, -0.7782394289970398, 0.2001725435256958, 1.49080228805542]], [[0.004831180442124605, -0.2068040370941162, 0.05714520439505577, 0.43072858452796936], [0.0006950994720682502, -0.0125358821824193, 0.06575977802276611, 0.15659357607364655]], [[-0.032370053231716156, 0.5549855828285217, 0.05745680257678032, -0.5242546796798706], [-0.02127034030854702, 0.7492538690567017, 0.04697170853614807, -0.7982932329177856]], [[-0.049195077270269394, -0.4065760672092438, 0.06388674676418304, 0.6115872859954834], [-0.05732659995555878, -0.6025300025939941, 0.07611849904060364, 0.9236884713172913]], [[-0.007450929377228022, -0.01027277484536171, -0.06650157272815704, -0.11155146360397339], [-0.007656384725123644, -0.20438189804553986, -0.0687326043844223, 0.1594315618276596]]]\n",
      "tensor([[[-1.1767e-01, -1.0220e+00,  1.5432e-01,  1.5287e+00],\n",
      "         [-1.3811e-01, -8.2906e-01,  1.8490e-01,  1.2879e+00]],\n",
      "\n",
      "        [[ 4.1135e-01,  8.8451e-01,  1.3011e-01, -4.3131e-02],\n",
      "         [ 4.2904e-01,  6.8779e-01,  1.2925e-01,  2.8761e-01]],\n",
      "\n",
      "        [[-6.0137e-02, -6.1221e-01, -2.0889e-03,  6.7476e-01],\n",
      "         [-7.2381e-02, -4.1706e-01,  1.1406e-02,  3.8142e-01]],\n",
      "\n",
      "        [[ 2.5346e-02, -2.4241e-01, -5.1788e-03,  2.9372e-01],\n",
      "         [ 2.0498e-02, -4.3746e-01,  6.9562e-04,  5.8477e-01]],\n",
      "\n",
      "        [[ 2.0498e-02, -4.3746e-01,  6.9562e-04,  5.8477e-01],\n",
      "         [ 1.1749e-02, -2.4235e-01,  1.2391e-02,  2.9230e-01]],\n",
      "\n",
      "        [[-2.1270e-02,  7.4925e-01,  4.6972e-02, -7.9829e-01],\n",
      "         [-6.2853e-03,  5.5352e-01,  3.1006e-02, -4.9121e-01]],\n",
      "\n",
      "        [[ 1.4743e-02, -1.5299e-01, -4.0171e-02,  2.8626e-01],\n",
      "         [ 1.1683e-02, -3.4752e-01, -3.4446e-02,  5.6601e-01]],\n",
      "\n",
      "        [[-3.6156e-02,  3.2211e-02, -1.6570e-02,  4.8606e-04],\n",
      "         [-3.5512e-02,  2.2757e-01, -1.6560e-02, -2.9738e-01]],\n",
      "\n",
      "        [[ 1.0657e-01, -3.2295e-01, -1.6154e-01, -5.7600e-03],\n",
      "         [ 1.0012e-01, -1.2593e-01, -1.6165e-01, -3.4474e-01]],\n",
      "\n",
      "        [[ 1.3050e-02,  6.1080e-01, -1.2024e-01, -1.0044e+00],\n",
      "         [ 2.5267e-02,  4.1748e-01, -1.4033e-01, -7.5180e-01]],\n",
      "\n",
      "        [[-5.3920e-03,  1.0292e-02,  3.4845e-02,  5.7377e-02],\n",
      "         [-5.1861e-03, -1.8531e-01,  3.5992e-02,  3.6085e-01]],\n",
      "\n",
      "        [[-1.2919e-01, -1.1990e+00,  7.7694e-02,  1.5876e+00],\n",
      "         [-1.5317e-01, -1.0049e+00,  1.0945e-01,  1.3201e+00]],\n",
      "\n",
      "        [[-4.4775e-02,  1.8261e-01,  1.8534e-03, -3.5464e-01],\n",
      "         [-4.1122e-02, -1.2537e-02, -5.2394e-03, -6.1372e-02]],\n",
      "\n",
      "        [[ 2.8951e-02, -1.5704e-01, -8.5550e-03,  3.0630e-01],\n",
      "         [ 2.5810e-02,  3.8199e-02, -2.4290e-03,  1.0932e-02]],\n",
      "\n",
      "        [[ 5.8192e-02,  4.3322e-01, -1.6257e-01, -8.3224e-01],\n",
      "         [ 6.6857e-02,  2.4065e-01, -1.7922e-01, -5.9477e-01]],\n",
      "\n",
      "        [[-4.1373e-02,  1.8266e-01, -6.4669e-03, -3.5570e-01],\n",
      "         [-3.7720e-02,  3.7787e-01, -1.3581e-02, -6.5042e-01]],\n",
      "\n",
      "        [[ 1.3252e-01,  1.0166e+00, -1.4857e-01, -1.5399e+00],\n",
      "         [ 1.5285e-01,  8.2357e-01, -1.7937e-01, -1.2970e+00]],\n",
      "\n",
      "        [[ 1.1973e-01,  2.4202e-01, -1.3352e-01, -4.7767e-01],\n",
      "         [ 1.2457e-01,  4.3875e-01, -1.4307e-01, -8.0927e-01]],\n",
      "\n",
      "        [[-4.7603e-02, -3.8001e-02,  1.1453e-03,  4.7707e-02],\n",
      "         [-4.8363e-02, -2.3314e-01,  2.0994e-03,  3.4075e-01]],\n",
      "\n",
      "        [[-1.4140e-01, -9.5382e-01,  1.5970e-01,  1.6078e+00],\n",
      "         [-1.6048e-01, -1.1504e+00,  1.9185e-01,  1.9457e+00]],\n",
      "\n",
      "        [[ 2.4934e-02,  2.8162e-02, -2.8236e-03,  4.6382e-02],\n",
      "         [ 2.5497e-02,  2.2332e-01, -1.8959e-03, -2.4719e-01]],\n",
      "\n",
      "        [[ 9.1999e-02,  4.2231e-01, -9.1793e-02, -6.2749e-01],\n",
      "         [ 1.0045e-01,  2.2858e-01, -1.0434e-01, -3.6507e-01]],\n",
      "\n",
      "        [[-6.2451e-03,  3.7939e-02, -6.1917e-03,  5.0820e-02],\n",
      "         [-5.4863e-03, -1.5709e-01, -5.1753e-03,  3.4154e-01]],\n",
      "\n",
      "        [[-4.0380e-02,  2.2826e-01, -5.3237e-02, -3.6597e-01],\n",
      "         [-3.5815e-02,  3.3930e-02, -6.0556e-02, -9.0537e-02]],\n",
      "\n",
      "        [[-1.2361e-01, -9.5360e-01,  1.7771e-01,  1.5079e+00],\n",
      "         [-1.4268e-01, -7.6103e-01,  2.0787e-01,  1.2755e+00]],\n",
      "\n",
      "        [[-1.2454e-01, -3.3427e-01, -6.3464e-02, -2.0845e-01],\n",
      "         [-1.3123e-01, -1.3830e-01, -6.7633e-02, -5.2046e-01]],\n",
      "\n",
      "        [[ 6.4332e-02, -1.8947e-01,  5.6863e-04,  2.3802e-01],\n",
      "         [ 6.0542e-02, -3.8460e-01,  5.3290e-03,  5.3088e-01]],\n",
      "\n",
      "        [[-8.3872e-02, -5.8130e-01,  1.7721e-01,  1.1482e+00],\n",
      "         [-9.5498e-02, -7.7824e-01,  2.0017e-01,  1.4908e+00]],\n",
      "\n",
      "        [[ 4.8312e-03, -2.0680e-01,  5.7145e-02,  4.3073e-01],\n",
      "         [ 6.9510e-04, -1.2536e-02,  6.5760e-02,  1.5659e-01]],\n",
      "\n",
      "        [[-3.2370e-02,  5.5499e-01,  5.7457e-02, -5.2425e-01],\n",
      "         [-2.1270e-02,  7.4925e-01,  4.6972e-02, -7.9829e-01]],\n",
      "\n",
      "        [[-4.9195e-02, -4.0658e-01,  6.3887e-02,  6.1159e-01],\n",
      "         [-5.7327e-02, -6.0253e-01,  7.6118e-02,  9.2369e-01]],\n",
      "\n",
      "        [[-7.4509e-03, -1.0273e-02, -6.6502e-02, -1.1155e-01],\n",
      "         [-7.6564e-03, -2.0438e-01, -6.8733e-02,  1.5943e-01]]])\n",
      "torch.Size([32, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
