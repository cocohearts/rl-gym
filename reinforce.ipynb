{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def get_action(self, x):\n",
    "        out = self(x)\n",
    "        return torch.argmax(out, dim=1)\n",
    "\n",
    "model = CartPoleModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 loss: -250.66653442382812\n",
      "Episode 0 reward: 9.380000114440918\n",
      "Episode 1 loss: -263.9714660644531\n",
      "Episode 1 reward: 9.388999938964844\n",
      "Episode 2 loss: -272.09307861328125\n",
      "Episode 2 reward: 9.32800006866455\n",
      "Episode 3 loss: -552.5525512695312\n",
      "Episode 3 reward: 9.99899959564209\n",
      "Episode 4 loss: -16533.763671875\n",
      "Episode 4 reward: 31.892000198364258\n",
      "Episode 5 loss: -5763.046875\n",
      "Episode 5 reward: 24.17099952697754\n",
      "Episode 6 loss: -3034.561279296875\n",
      "Episode 6 reward: 18.097999572753906\n",
      "Episode 7 loss: -373.2899475097656\n",
      "Episode 7 reward: 9.531999588012695\n",
      "Episode 8 loss: -299.8049011230469\n",
      "Episode 8 reward: 9.35099983215332\n",
      "Episode 9 loss: -299.3340759277344\n",
      "Episode 9 reward: 9.371000289916992\n",
      "Episode 10 loss: -297.77880859375\n",
      "Episode 10 reward: 9.362000465393066\n",
      "Episode 11 loss: -301.0747375488281\n",
      "Episode 11 reward: 9.369000434875488\n",
      "Episode 12 loss: -303.4683837890625\n",
      "Episode 12 reward: 9.359999656677246\n",
      "Episode 13 loss: -307.1150817871094\n",
      "Episode 13 reward: 9.348999977111816\n",
      "Episode 14 loss: -615.2040405273438\n",
      "Episode 14 reward: 10.204999923706055\n",
      "Episode 15 loss: -2412.83740234375\n",
      "Episode 15 reward: 15.996000289916992\n",
      "Episode 16 loss: -4421.4150390625\n",
      "Episode 16 reward: 21.509000778198242\n",
      "Episode 17 loss: -3666.64111328125\n",
      "Episode 17 reward: 19.13599967956543\n",
      "Episode 18 loss: -1873.2589111328125\n",
      "Episode 18 reward: 14.0649995803833\n",
      "Episode 19 loss: -707.431884765625\n",
      "Episode 19 reward: 10.722999572753906\n",
      "Episode 20 loss: -415.9331359863281\n",
      "Episode 20 reward: 9.777999877929688\n",
      "Episode 21 loss: -396.0947570800781\n",
      "Episode 21 reward: 9.807000160217285\n",
      "Episode 22 loss: -404.74493408203125\n",
      "Episode 22 reward: 10.069000244140625\n",
      "Episode 23 loss: -547.8822631835938\n",
      "Episode 23 reward: 10.940999984741211\n",
      "Episode 24 loss: -669.936767578125\n",
      "Episode 24 reward: 11.57699966430664\n",
      "Episode 25 loss: -905.2628784179688\n",
      "Episode 25 reward: 12.616000175476074\n",
      "Episode 26 loss: -1144.761962890625\n",
      "Episode 26 reward: 13.586999893188477\n",
      "Episode 27 loss: -1476.6658935546875\n",
      "Episode 27 reward: 14.595000267028809\n",
      "Episode 28 loss: -1631.1802978515625\n",
      "Episode 28 reward: 15.086000442504883\n",
      "Episode 29 loss: -1516.774658203125\n",
      "Episode 29 reward: 14.774999618530273\n",
      "Episode 30 loss: -1299.274169921875\n",
      "Episode 30 reward: 14.057999610900879\n",
      "Episode 31 loss: -1007.8351440429688\n",
      "Episode 31 reward: 13.177000045776367\n",
      "Episode 32 loss: -768.5848388671875\n",
      "Episode 32 reward: 12.234000205993652\n",
      "Episode 33 loss: -607.5993041992188\n",
      "Episode 33 reward: 11.434000015258789\n",
      "Episode 34 loss: -542.2538452148438\n",
      "Episode 34 reward: 11.015000343322754\n",
      "Episode 35 loss: -505.50146484375\n",
      "Episode 35 reward: 10.739999771118164\n",
      "Episode 36 loss: -514.6046142578125\n",
      "Episode 36 reward: 10.694999694824219\n",
      "Episode 37 loss: -525.971435546875\n",
      "Episode 37 reward: 10.777999877929688\n",
      "Episode 38 loss: -541.0155029296875\n",
      "Episode 38 reward: 10.902000427246094\n",
      "Episode 39 loss: -564.724365234375\n",
      "Episode 39 reward: 11.07800006866455\n",
      "Episode 40 loss: -554.731201171875\n",
      "Episode 40 reward: 11.019000053405762\n",
      "Episode 41 loss: -578.2681274414062\n",
      "Episode 41 reward: 11.163999557495117\n",
      "Episode 42 loss: -552.8431396484375\n",
      "Episode 42 reward: 11.005999565124512\n",
      "Episode 43 loss: -517.541259765625\n",
      "Episode 43 reward: 10.758999824523926\n",
      "Episode 44 loss: -518.807373046875\n",
      "Episode 44 reward: 10.75100040435791\n",
      "Episode 45 loss: -503.8633117675781\n",
      "Episode 45 reward: 10.678999900817871\n",
      "Episode 46 loss: -472.3323669433594\n",
      "Episode 46 reward: 10.425000190734863\n",
      "Episode 47 loss: -479.5978698730469\n",
      "Episode 47 reward: 10.510000228881836\n",
      "Episode 48 loss: -445.0173034667969\n",
      "Episode 48 reward: 10.32800006866455\n",
      "Episode 49 loss: -489.6533508300781\n",
      "Episode 49 reward: 10.668000221252441\n",
      "Episode 50 loss: -568.3133544921875\n",
      "Episode 50 reward: 11.12600040435791\n",
      "Episode 51 loss: -641.0097045898438\n",
      "Episode 51 reward: 11.482999801635742\n",
      "Episode 52 loss: -733.7889404296875\n",
      "Episode 52 reward: 11.916999816894531\n",
      "Episode 53 loss: -862.318359375\n",
      "Episode 53 reward: 12.387999534606934\n",
      "Episode 54 loss: -908.8187866210938\n",
      "Episode 54 reward: 12.571999549865723\n",
      "Episode 55 loss: -1010.1439208984375\n",
      "Episode 55 reward: 12.812000274658203\n",
      "Episode 56 loss: -956.2664184570312\n",
      "Episode 56 reward: 12.579999923706055\n",
      "Episode 57 loss: -1061.2601318359375\n",
      "Episode 57 reward: 12.708000183105469\n",
      "Episode 58 loss: -926.4078369140625\n",
      "Episode 58 reward: 12.170999526977539\n",
      "Episode 59 loss: -998.3900146484375\n",
      "Episode 59 reward: 12.416000366210938\n",
      "Episode 60 loss: -966.0856323242188\n",
      "Episode 60 reward: 12.057000160217285\n",
      "Episode 61 loss: -1170.4940185546875\n",
      "Episode 61 reward: 12.376999855041504\n",
      "Episode 62 loss: -1469.213134765625\n",
      "Episode 62 reward: 13.246999740600586\n",
      "Episode 63 loss: -2359.070556640625\n",
      "Episode 63 reward: 14.520999908447266\n",
      "Episode 64 loss: -3310.932861328125\n",
      "Episode 64 reward: 14.854999542236328\n",
      "Episode 65 loss: -5194.89794921875\n",
      "Episode 65 reward: 16.034000396728516\n",
      "Episode 66 loss: -5802.5654296875\n",
      "Episode 66 reward: 15.413000106811523\n",
      "Episode 67 loss: -6199.7587890625\n",
      "Episode 67 reward: 14.684000015258789\n",
      "Episode 68 loss: -5823.6767578125\n",
      "Episode 68 reward: 16.48699951171875\n",
      "Episode 69 loss: -16764.306640625\n",
      "Episode 69 reward: 29.26799964904785\n",
      "Episode 70 loss: -3255.539794921875\n",
      "Episode 70 reward: 16.52899932861328\n",
      "Episode 71 loss: -533.6717529296875\n",
      "Episode 71 reward: 10.947999954223633\n",
      "Episode 72 loss: -372.24560546875\n",
      "Episode 72 reward: 9.805999755859375\n",
      "Episode 73 loss: -332.52764892578125\n",
      "Episode 73 reward: 9.5\n",
      "Episode 74 loss: -333.49676513671875\n",
      "Episode 74 reward: 9.501999855041504\n",
      "Episode 75 loss: -375.9320983886719\n",
      "Episode 75 reward: 9.843999862670898\n",
      "Episode 76 loss: -480.312255859375\n",
      "Episode 76 reward: 10.618000030517578\n",
      "Episode 77 loss: -465.7584228515625\n",
      "Episode 77 reward: 10.460000038146973\n",
      "Episode 78 loss: -459.3633728027344\n",
      "Episode 78 reward: 10.104000091552734\n",
      "Episode 79 loss: -660.6402587890625\n",
      "Episode 79 reward: 10.36400032043457\n",
      "Episode 80 loss: -1171.260009765625\n",
      "Episode 80 reward: 11.807000160217285\n",
      "Episode 81 loss: -7349.1748046875\n",
      "Episode 81 reward: 22.47599983215332\n",
      "Episode 82 loss: -938.473388671875\n",
      "Episode 82 reward: 13.26200008392334\n",
      "Episode 83 loss: -576.9171752929688\n",
      "Episode 83 reward: 11.336000442504883\n",
      "Episode 84 loss: -447.8407287597656\n",
      "Episode 84 reward: 10.446999549865723\n",
      "Episode 85 loss: -385.20513916015625\n",
      "Episode 85 reward: 9.944000244140625\n",
      "Episode 86 loss: -379.9308776855469\n",
      "Episode 86 reward: 9.91100025177002\n",
      "Episode 87 loss: -434.0933837890625\n",
      "Episode 87 reward: 10.329000473022461\n",
      "Episode 88 loss: -521.3577880859375\n",
      "Episode 88 reward: 10.972000122070312\n",
      "Episode 89 loss: -746.2869873046875\n",
      "Episode 89 reward: 12.388999938964844\n",
      "Episode 90 loss: -1067.969482421875\n",
      "Episode 90 reward: 13.854000091552734\n",
      "Episode 91 loss: -2097.947998046875\n",
      "Episode 91 reward: 17.334999084472656\n",
      "Episode 92 loss: -11301.669921875\n",
      "Episode 92 reward: 25.226999282836914\n",
      "Episode 93 loss: -20698.630859375\n",
      "Episode 93 reward: 30.863000869750977\n",
      "Episode 94 loss: -1643.143798828125\n",
      "Episode 94 reward: 12.923999786376953\n",
      "Episode 95 loss: -626.1593627929688\n",
      "Episode 95 reward: 10.854000091552734\n",
      "Episode 96 loss: -580.5863037109375\n",
      "Episode 96 reward: 11.032999992370605\n",
      "Episode 97 loss: -456.2964172363281\n",
      "Episode 97 reward: 10.37399959564209\n",
      "Episode 98 loss: -424.67706298828125\n",
      "Episode 98 reward: 10.204999923706055\n",
      "Episode 99 loss: -393.68719482421875\n",
      "Episode 99 reward: 9.994999885559082\n",
      "Episode 100 loss: -349.3984680175781\n",
      "Episode 100 reward: 9.619999885559082\n",
      "Episode 101 loss: -313.4932861328125\n",
      "Episode 101 reward: 9.3149995803833\n",
      "Episode 102 loss: -294.00469970703125\n",
      "Episode 102 reward: 9.144000053405762\n",
      "Episode 103 loss: -275.9893798828125\n",
      "Episode 103 reward: 8.991999626159668\n",
      "Episode 104 loss: -266.8814697265625\n",
      "Episode 104 reward: 8.9350004196167\n",
      "Episode 105 loss: -271.595947265625\n",
      "Episode 105 reward: 9.027999877929688\n",
      "Episode 106 loss: -275.1177673339844\n",
      "Episode 106 reward: 9.083000183105469\n",
      "Episode 107 loss: -276.7892150878906\n",
      "Episode 107 reward: 9.104000091552734\n",
      "Episode 108 loss: -289.14703369140625\n",
      "Episode 108 reward: 9.208999633789062\n",
      "Episode 109 loss: -304.1534118652344\n",
      "Episode 109 reward: 9.324000358581543\n",
      "Episode 110 loss: -340.8621520996094\n",
      "Episode 110 reward: 9.590999603271484\n",
      "Episode 111 loss: -405.7565612792969\n",
      "Episode 111 reward: 10.067000389099121\n",
      "Episode 112 loss: -484.03985595703125\n",
      "Episode 112 reward: 10.590999603271484\n",
      "Episode 113 loss: -581.40673828125\n",
      "Episode 113 reward: 11.21399974822998\n",
      "Episode 114 loss: -673.7080078125\n",
      "Episode 114 reward: 11.713000297546387\n",
      "Episode 115 loss: -725.1963500976562\n",
      "Episode 115 reward: 11.95300006866455\n",
      "Episode 116 loss: -789.1088256835938\n",
      "Episode 116 reward: 12.265000343322754\n",
      "Episode 117 loss: -798.8162231445312\n",
      "Episode 117 reward: 12.326000213623047\n",
      "Episode 118 loss: -757.3311157226562\n",
      "Episode 118 reward: 12.175999641418457\n",
      "Episode 119 loss: -693.0660400390625\n",
      "Episode 119 reward: 11.937000274658203\n",
      "Episode 120 loss: -702.7401123046875\n",
      "Episode 120 reward: 12.069000244140625\n",
      "Episode 121 loss: -741.2843017578125\n",
      "Episode 121 reward: 12.359000205993652\n",
      "Episode 122 loss: -747.6213989257812\n",
      "Episode 122 reward: 12.359999656677246\n",
      "Episode 123 loss: -785.3519287109375\n",
      "Episode 123 reward: 12.503999710083008\n",
      "Episode 124 loss: -807.5298461914062\n",
      "Episode 124 reward: 12.626999855041504\n",
      "Episode 125 loss: -914.275634765625\n",
      "Episode 125 reward: 13.006999969482422\n",
      "Episode 126 loss: -965.8688354492188\n",
      "Episode 126 reward: 13.260000228881836\n",
      "Episode 127 loss: -1078.6251220703125\n",
      "Episode 127 reward: 13.730999946594238\n",
      "Episode 128 loss: -1110.290771484375\n",
      "Episode 128 reward: 13.916000366210938\n",
      "Episode 129 loss: -1206.6824951171875\n",
      "Episode 129 reward: 14.288999557495117\n",
      "Episode 130 loss: -1177.5447998046875\n",
      "Episode 130 reward: 14.22700023651123\n",
      "Episode 131 loss: -1103.882080078125\n",
      "Episode 131 reward: 13.937999725341797\n",
      "Episode 132 loss: -985.7097778320312\n",
      "Episode 132 reward: 13.480999946594238\n",
      "Episode 133 loss: -898.2684936523438\n",
      "Episode 133 reward: 13.109000205993652\n",
      "Episode 134 loss: -828.9786987304688\n",
      "Episode 134 reward: 12.779999732971191\n",
      "Episode 135 loss: -802.5958862304688\n",
      "Episode 135 reward: 12.647000312805176\n",
      "Episode 136 loss: -762.09912109375\n",
      "Episode 136 reward: 12.428999900817871\n",
      "Episode 137 loss: -751.2048950195312\n",
      "Episode 137 reward: 12.38700008392334\n",
      "Episode 138 loss: -735.4592895507812\n",
      "Episode 138 reward: 12.300000190734863\n",
      "Episode 139 loss: -735.742919921875\n",
      "Episode 139 reward: 12.277999877929688\n",
      "Episode 140 loss: -738.1438598632812\n",
      "Episode 140 reward: 12.276000022888184\n",
      "Episode 141 loss: -812.0150756835938\n",
      "Episode 141 reward: 12.539999961853027\n",
      "Episode 142 loss: -905.1283569335938\n",
      "Episode 142 reward: 12.904999732971191\n",
      "Episode 143 loss: -977.0248413085938\n",
      "Episode 143 reward: 13.229000091552734\n",
      "Episode 144 loss: -1075.7349853515625\n",
      "Episode 144 reward: 13.583000183105469\n",
      "Episode 145 loss: -1222.6103515625\n",
      "Episode 145 reward: 14.107999801635742\n",
      "Episode 146 loss: -1275.4078369140625\n",
      "Episode 146 reward: 14.343999862670898\n",
      "Episode 147 loss: -1444.998779296875\n",
      "Episode 147 reward: 14.79699993133545\n",
      "Episode 148 loss: -1673.7203369140625\n",
      "Episode 148 reward: 15.402999877929688\n",
      "Episode 149 loss: -1960.706787109375\n",
      "Episode 149 reward: 15.88599967956543\n",
      "Episode 150 loss: -2080.17822265625\n",
      "Episode 150 reward: 16.347999572753906\n",
      "Episode 151 loss: -2316.337646484375\n",
      "Episode 151 reward: 16.990999221801758\n",
      "Episode 152 loss: -2345.686279296875\n",
      "Episode 152 reward: 16.98900032043457\n",
      "Episode 153 loss: -2248.456298828125\n",
      "Episode 153 reward: 16.566999435424805\n",
      "Episode 154 loss: -2112.529296875\n",
      "Episode 154 reward: 15.998000144958496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):  \u001b[38;5;66;03m# Still run 10 episodes per batch\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     loss, total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     episode_losses[i] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     30\u001b[0m     total_rewards[i] \u001b[38;5;241m=\u001b[39m total_reward\n",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[1;32m     10\u001b[0m     obs_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(obs_input)\n\u001b[1;32m     13\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(out[:, action])\n",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m, in \u001b[0;36mCartPoleModel.get_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36mCartPoleModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tor_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tor_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_episode(env_name=\"CartPole-v1\"):\n",
    "    env = gym.make(env_name)  # Remove render_mode for training\n",
    "    episode = torch.tensor([])\n",
    "    observation, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        obs_input = torch.tensor(observation, dtype=torch.float32)[None, :]\n",
    "        action = model.get_action(obs_input)\n",
    "        out = model(obs_input)\n",
    "        log_prob = torch.log(out[:, action])\n",
    "        observation, reward, terminated, truncated, info = env.step(action.numpy()[0])\n",
    "        total_reward += reward\n",
    "        episode = torch.cat([episode, log_prob])\n",
    "    \n",
    "    rewards_to_go = torch.arange(len(episode), 0, -1)\n",
    "    episode_loss = (rewards_to_go * episode).sum() # Negative since we want to maximize reward\n",
    "    env.close()\n",
    "    return (episode_loss, total_reward)\n",
    "\n",
    "# Run episodes sequentially instead of in parallel\n",
    "for episode_num in tqdm(range(1000)):\n",
    "    episode_losses = torch.zeros(1000)\n",
    "    total_rewards = torch.zeros(1000)\n",
    "    for i in range(1000):  # Still run 10 episodes per batch\n",
    "        loss, total_reward = run_episode()\n",
    "        episode_losses[i] = loss\n",
    "        total_rewards[i] = total_reward\n",
    "    \n",
    "    # Sum up losses from all episodes\n",
    "    total_loss = episode_losses.mean()\n",
    "    total_reward = total_rewards.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    # Keep track of last 5 episodes in output\n",
    "    if episode_num >= 5:\n",
    "        print(\"\\033[F\\033[K\", end=\"\")  # Clear the first line\n",
    "        print(\"\\033[F\\033[K\", end=\"\")  # Clear the second line\n",
    "    print(f\"Episode {episode_num} loss: {total_loss.item()}\")\n",
    "    print(f\"Episode {episode_num} reward: {total_rewards.mean().item()}\")\n",
    "    print(f\"Episode {episode_num} avg reward over last 100: {total_rewards[-100:].mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
